#!/usr/bin/env python3
"""
å®éªŒä¸»è„šæœ¬ - å¼‚æ­¥ç‰ˆæœ¬ + MLflow è¿½è¸ª
"""
import argparse
import asyncio
import os
import sys
from pathlib import Path
import logging
import mlflow

PROJECT_ROOT = Path(__file__).parent.absolute()
sys.path.insert(0, str(PROJECT_ROOT))

from utils.io_handler import (
    load_questions,
    save_json,
    format_generation_output,
    format_scoring_output,
    format_final_output
)
from pipeline.generation_async import step1_qwen_generation_async
from pipeline.scoring_async import step2_gpt_scoring_async
from pipeline.selection import step3_selection

def setup_logger(log_file: str = None):
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    if log_file:
        Path(log_file).parent.mkdir(parents=True, exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8') if log_file else logging.NullHandler(),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger('experiment')

async def main_async(args):
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    # é…ç½® MLflow
    mlflow.set_tracking_uri("sqlite:///mlflow.db")
    mlflow.set_experiment("ESC_Experiments")
    
    # å¯åŠ¨ MLflow run
    with mlflow.start_run(run_name=args.version):
        # è®¾ç½®è¾“å‡ºç›®å½•
        output_dir = PROJECT_ROOT / 'Outputs' / args.version
        output_dir.mkdir(parents=True, exist_ok=True)
        output_dir = str(output_dir)
        
        # è®¾ç½®æ—¥å¿—
        if args.log is None:
            args.log = os.path.join(output_dir, f'experiment_{args.version}.log')
        logger = setup_logger(args.log)
        
        # æ—¥å¿—å¤´
        logger.info("="*80)
        logger.info(f"ğŸ§ª å®éªŒé…ç½® [{args.version}] - å¼‚æ­¥ç‰ˆæœ¬ + MLflow")
        logger.info("="*80)
        logger.info(f"è¾“å…¥: {args.input} | é—®é¢˜æ•°: {args.limit}")
        logger.info(f"å€™é€‰æ•°: {args.candidates} | è¯„åˆ†è½®æ¬¡: {args.score_rounds} | Top-K: {args.top_k}")
        logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
        logger.info("="*80)
        
        # è®°å½•å®éªŒå‚æ•°åˆ° MLflow
        mlflow.log_params({
            "version": args.version,
            "limit": args.limit,
            "candidates": args.candidates,
            "score_rounds": args.score_rounds,
            "top_k": args.top_k,
            "input_file": args.input
        })
        
        # åŠ è½½æ•°æ®
        questions = load_questions(args.input, args.limit)
        logger.info(f"\nâœ… å·²åŠ è½½ {len(questions)} ä¸ªé—®é¢˜")
        mlflow.log_metric("num_questions", len(questions))
        
        # Step 1: Qwenç”Ÿæˆ (å¼‚æ­¥)
        candidates = await step1_qwen_generation_async(questions, args.candidates)
        
        # ä¿å­˜åŸå§‹ç”Ÿæˆç»“æœ
        raw_file = os.path.join(output_dir, f"qwen_candidates_raw_{args.version}.json")
        save_json(candidates, raw_file)
        logger.info(f"ğŸ’¾ å·²ä¿å­˜åŸå§‹æ•°æ®: {raw_file}")
        
        # ä¿å­˜æ ¼å¼åŒ–çš„ç”Ÿæˆç»“æœ
        formatted_gen = format_generation_output(candidates)
        gen_file = os.path.join(output_dir, f"1_generation_{args.version}.json")
        save_json(formatted_gen, gen_file)
        logger.info(f"ğŸ’¾ å·²ä¿å­˜ç”Ÿæˆç»“æœ: {gen_file}")
        mlflow.log_metric("num_candidates_generated", len(candidates))
        
        # Step 2: GPTè¯„åˆ† (å¼‚æ­¥)
        scored_candidates = await step2_gpt_scoring_async(candidates, args.score_rounds)
        
        # ä¿å­˜åŸå§‹è¯„åˆ†ç»“æœ
        raw_scores_file = os.path.join(output_dir, f"gpt_scores_raw_{args.version}.json")
        save_json(scored_candidates, raw_scores_file)
        logger.info(f"ğŸ’¾ å·²ä¿å­˜åŸå§‹è¯„åˆ†: {raw_scores_file}")
        
        # ä¿å­˜æ ¼å¼åŒ–çš„è¯„åˆ†ç»“æœ
        formatted_scores = format_scoring_output(scored_candidates)
        scores_file = os.path.join(output_dir, f"2_scores_{args.version}.json")
        save_json(formatted_scores, scores_file)
        logger.info(f"ğŸ’¾ å·²ä¿å­˜è¯„åˆ†ç»“æœ: {scores_file}")
        
        # è®¡ç®—å¹¶è®°å½•å¹³å‡åˆ†æ•°
        if scored_candidates:
            avg_empathy = sum(c['scores']['Empathy'] for c in scored_candidates) / len(scored_candidates)
            avg_supportiveness = sum(c['scores']['Supportiveness'] for c in scored_candidates) / len(scored_candidates)
            avg_guidance = sum(c['scores']['Guidance'] for c in scored_candidates) / len(scored_candidates)
            avg_safety = sum(c['scores']['Safety'] for c in scored_candidates) / len(scored_candidates)
            avg_total = sum(c['scores']['Total'] for c in scored_candidates) / len(scored_candidates)
            
            mlflow.log_metrics({
                "avg_empathy": avg_empathy,
                "avg_supportiveness": avg_supportiveness,
                "avg_guidance": avg_guidance,
                "avg_safety": avg_safety,
                "avg_total_score": avg_total
            })
            
            logger.info(f"\nğŸ“Š å¹³å‡åˆ†æ•°:")
            logger.info(f"  Empathy: {avg_empathy:.2f}")
            logger.info(f"  Supportiveness: {avg_supportiveness:.2f}")
            logger.info(f"  Guidance: {avg_guidance:.2f}")
            logger.info(f"  Safety: {avg_safety:.2f}")
            logger.info(f"  Total: {avg_total:.2f}")
        
        # Step 3: ç”Ÿæˆæœ€ç»ˆç»“æœ
        final_results = format_final_output(scored_candidates)
        final_file = os.path.join(output_dir, f"3_final_results_{args.version}.json")
        save_json(final_results, final_file)
        logger.info(f"ğŸ’¾ å·²ä¿å­˜æœ€ç»ˆç»“æœ: {final_file}")
        mlflow.log_metric("num_final_results", len(final_results))
        
        # è®°å½•è¾“å…¥è¾“å‡ºæ–‡ä»¶åˆ° MLflow
        mlflow.log_artifact(args.input, artifact_path="inputs")
        mlflow.log_artifacts(output_dir, artifact_path="outputs")
        
        # å®Œæˆ
        logger.info("\n" + "="*80)
        logger.info("ğŸ‰ å®éªŒå®Œæˆï¼")
        logger.info("="*80)
        logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"ğŸ“Š MLflow Run ID: {mlflow.active_run().info.run_id}")
        logger.info("="*80)

def main():
    parser = argparse.ArgumentParser(description='è¿è¡Œå®éªŒ - å¼‚æ­¥ç‰ˆæœ¬ + MLflow')
    parser.add_argument('--limit', type=int, default=10, help='é—®é¢˜æ•°é‡é™åˆ¶')
    parser.add_argument('--candidates', type=int, default=2, help='æ¯æ¡é—®é¢˜ç”Ÿæˆå€™é€‰æ•°')
    parser.add_argument('--score-rounds', type=int, default=3, help='æ¯ä¸ªå€™é€‰è¯„åˆ†æ¬¡æ•°')
    parser.add_argument('--version', type=str, default='v1_async', help='å®éªŒç‰ˆæœ¬å·')
    parser.add_argument('--top-k', type=int, default=5, help='é€‰æ‹©Top-K')
    parser.add_argument('--input', type=str, default=str(PROJECT_ROOT / 'inputs' / 'questions.txt'), help='è¾“å…¥æ–‡ä»¶')
    parser.add_argument('--log', type=str, default=None, help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main_async(args))

if __name__ == "__main__":
    main()