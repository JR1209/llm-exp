#!/usr/bin/env python3
"""
å®éªŒä¸»è„šæœ¬ - å¼‚æ­¥ç‰ˆæœ¬
"""

import argparse
import asyncio
import os
import sys
from pathlib import Path
import logging

PROJECT_ROOT = Path(__file__).parent.absolute()
sys.path.insert(0, str(PROJECT_ROOT))

from utils.io_handler import load_questions, save_json
from pipeline.generation_async import step1_qwen_generation_async
from pipeline.scoring_async import step2_gpt_scoring_async
from pipeline.selection import step3_selection


def setup_logger(log_file: str = None):
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    if log_file:
        Path(log_file).parent.mkdir(parents=True, exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8') if log_file else logging.NullHandler(),
            logging.StreamHandler()
        ]
    )
    
    return logging.getLogger('experiment')


async def main_async(args):
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_dir = PROJECT_ROOT / 'Outputs' / args.version
    output_dir.mkdir(parents=True, exist_ok=True)
    output_dir = str(output_dir)
    
    # è®¾ç½®æ—¥å¿—
    if args.log is None:
        args.log = os.path.join(output_dir, f'experiment_{args.version}.log')
    
    logger = setup_logger(args.log)
    
    # æ—¥å¿—å¤´
    logger.info("="*80)
    logger.info(f"ğŸ§ª å®éªŒé…ç½® [{args.version}] - å¼‚æ­¥ç‰ˆæœ¬")
    logger.info("="*80)
    logger.info(f"è¾“å…¥: {args.input} | é—®é¢˜æ•°: {args.limit}")
    logger.info(f"å€™é€‰æ•°: {args.candidates} | è¯„åˆ†è½®æ¬¡: {args.score_rounds} | Top-K: {args.top_k}")
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    logger.info("="*80)
    
    # åŠ è½½æ•°æ®
    questions = load_questions(args.input, args.limit)
    logger.info(f"\nâœ… å·²åŠ è½½ {len(questions)} ä¸ªé—®é¢˜")
    
    # Step 1: Qwenç”Ÿæˆ (å¼‚æ­¥)
    candidates = await step1_qwen_generation_async(questions, args.candidates)
    output_file = os.path.join(output_dir, f"qwen_candidates_{args.version}.json")
    save_json(candidates, output_file)
    logger.info(f"ğŸ’¾ å·²ä¿å­˜: {output_file}")
    
    # Step 2: GPTè¯„åˆ† (å¼‚æ­¥)
    scored_candidates = await step2_gpt_scoring_async(candidates, args.score_rounds)
    output_file = os.path.join(output_dir, f"gpt_scores_{args.version}.json")
    save_json(scored_candidates, output_file)
    logger.info(f"ğŸ’¾ å·²ä¿å­˜: {output_file}")
    
    # Step 3: é€‰æ‹©Top-K (åŒæ­¥,ä¸éœ€è¦æ”¹)
    top_results = step3_selection(scored_candidates, args.top_k)
    output_file = os.path.join(output_dir, f"top_results_{args.version}.json")
    save_json(top_results, output_file)
    logger.info(f"ğŸ’¾ å·²ä¿å­˜: {output_file}")
    
    # å®Œæˆ
    logger.info("\n" + "="*80)
    logger.info("ğŸ‰ å®éªŒå®Œæˆï¼")
    logger.info("="*80)
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    logger.info(f"  - qwen_candidates_{args.version}.json")
    logger.info(f"  - gpt_scores_{args.version}.json")
    logger.info(f"  - top_results_{args.version}.json")
    logger.info(f"  - experiment_{args.version}.log")
    logger.info("="*80)


def main():
    parser = argparse.ArgumentParser(description='è¿è¡Œå®éªŒ - å¼‚æ­¥ç‰ˆæœ¬')
    parser.add_argument('--limit', type=int, default=10, help='é—®é¢˜æ•°é‡é™åˆ¶')
    parser.add_argument('--candidates', type=int, default=2, help='æ¯æ¡é—®é¢˜ç”Ÿæˆå€™é€‰æ•°')
    parser.add_argument('--score-rounds', type=int, default=3, help='æ¯ä¸ªå€™é€‰è¯„åˆ†æ¬¡æ•°')
    parser.add_argument('--version', type=str, default='v1_async', help='å®éªŒç‰ˆæœ¬å·')
    parser.add_argument('--top-k', type=int, default=5, help='é€‰æ‹©Top-K')
    parser.add_argument('--input', type=str, default=str(PROJECT_ROOT / 'inputs' / 'questions.txt'), help='è¾“å…¥æ–‡ä»¶')
    parser.add_argument('--log', type=str, default=None, help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main_async(args))


if __name__ == "__main__":
    main()
