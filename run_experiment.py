#!/usr/bin/env python3
"""
å®éªŒä¸»è„šæœ¬ï¼šä¸‰æ­¥å›ºå®šæµç¨‹
Step 1: Qwenæ‰¹é‡ç”Ÿæˆ
Step 2: GPTå¤šè½®è¯„åˆ†
Step 3: é€‰æ‹©Top-K
"""

import json
import requests
import argparse
import time
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
import yaml

with open("params.yaml") as f:
    params = yaml.safe_load(f)

prompt_version = params["prompt_version"]

from config import (
    API_KEY, API_BASE_URL,
    QWEN_MODEL, GPT_MODEL,
    QWEN_GENERATION_PROMPT,
    GPT_SCORING_PROMPT
)


# ============================================
# æ—¥å¿—é…ç½®
# ============================================

def setup_logger(log_file: str = "experiment.log"):
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ - è¡¨æ ¼åŒ–è¾“å‡º"""
    Path(log_file).parent.mkdir(parents=True, exist_ok=True)
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(message)s',  # ç®€åŒ–æ ¼å¼
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    return logger

# å…¨å±€logger
logger = None


# ============================================
# APIè°ƒç”¨
# ============================================

def call_api(model: str, prompt: str, max_retries: int = 3) -> str:
    """è°ƒç”¨API"""
    url = f"{API_BASE_URL}/chat/completions"
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.post(
                url,
                headers=headers,
                json={
                    "model": model,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.7,
                    "max_tokens": 2000
                },
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()["choices"][0]["message"]["content"]
                if logger:
                    logger.debug(f"APIè°ƒç”¨æˆåŠŸ [{model}]: {len(result)} å­—ç¬¦")
                return result
            else:
                if logger:
                    logger.warning(f"APIè°ƒç”¨å¤±è´¥ [{model}]: HTTP {response.status_code}")
            
            if attempt < max_retries - 1:
                time.sleep(2)
                
        except Exception as e:
            if logger:
                logger.warning(f"APIè°ƒç”¨å¼‚å¸¸ [{model}]: {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
    
    if logger:
        logger.error(f"APIè°ƒç”¨æœ€ç»ˆå¤±è´¥ [{model}]")
    return ""


# ============================================
# æ•°æ®åŠ è½½
# ============================================

def load_questions(file_path: str, limit: int) -> List[str]:
    """åŠ è½½é—®é¢˜ï¼ˆé™åˆ¶æ•°é‡ï¼‰"""
    questions = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= limit:
                break
            q = line.strip()
            if q:
                questions.append(q)
    return questions


def save_jsonl(data: List[Dict], file_path: str):
    """ä¿å­˜JSONL"""
    Path(file_path).parent.mkdir(parents=True, exist_ok=True)
    with open(file_path, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    if logger:
        logger.info(f"ğŸ’¾ å·²ä¿å­˜: {file_path}")


# ============================================
# Step 1: Qwenæ‰¹é‡ç”Ÿæˆ
# ============================================

def step1_qwen_generation(questions: List[str], num_candidates: int, output_file: str):
    """Step 1: ä½¿ç”¨Qwenå¹¶è¡Œç”Ÿæˆå€™é€‰å¯¹è¯"""
    logger.info("\n" + "="*80)
    logger.info("Step 1: Qwen Batch Generation (Parallel)")
    logger.info("="*80)
    logger.info(f"é—®é¢˜æ•°: {len(questions)} | æ¯é¢˜å€™é€‰: {num_candidates} | å¹¶è¡Œæ•°: 100")
    logger.info(f"\n{'QID':<5} {'CID':<5} {'Status':<10} {'Length':<10}")
    logger.info("-"*80)
    
    results = []
    tasks = []
    
    # æ„å»ºä»»åŠ¡åˆ—è¡¨
    for idx, question in enumerate(questions, 1):
        for cand_idx in range(num_candidates):
            tasks.append((idx, question, cand_idx + 1))
    
    # å¹¶è¡Œæ‰§è¡Œ
    def generate_one(task):
        idx, question, cand_id = task
        prompt = QWEN_GENERATION_PROMPT.format(question=question)
        dialogue = call_api(QWEN_MODEL, prompt)
        return idx, question, cand_id, dialogue
    
    with ThreadPoolExecutor(max_workers=100) as executor:
        futures = {executor.submit(generate_one, task): task for task in tasks}
        
        for future in as_completed(futures):
            idx, question, cand_id, dialogue = future.result()
            if dialogue:
                results.append({
                    "question_id": idx,
                    "question": question,
                    "candidate_id": cand_id,
                    "dialogue": dialogue,
                    "model": QWEN_MODEL
                })
                logger.info(f"{idx:<5} {cand_id:<5} {'âœ“ Success':<10} {len(dialogue):<10}")
            else:
                logger.info(f"{idx:<5} {cand_id:<5} {'âœ— Failed':<10} {0:<10}")
    
    save_jsonl(results, output_file)
    logger.info("-"*80)
    logger.info(f"âœ… Step 1 å®Œæˆ: {len(results)}/{len(tasks)} æˆåŠŸ\n")
    return results


# ============================================
# Step 2: GPTå¤šè½®è¯„åˆ†
# ============================================

def parse_scores(score_text: str) -> Dict[str, float]:
    """è§£æè¯„åˆ†æ–‡æœ¬"""
    scores = {}
    for line in score_text.split('\n'):
        line = line.strip()
        if ':' in line:
            key, value = line.split(':', 1)
            key = key.strip()
            try:
                scores[key] = float(value.strip())
            except:
                scores[key] = 0.0
    return scores


def step2_gpt_scoring(candidates: List[Dict], num_rounds: int, output_file: str):
    """Step 2: ä½¿ç”¨GPTå¹¶è¡Œè¯„åˆ†"""
    logger.info("\n" + "="*80)
    logger.info("Step 2: GPT Multi-round Scoring (Parallel)")
    logger.info("="*80)
    logger.info(f"å€™é€‰æ•°: {len(candidates)} | è¯„åˆ†è½®æ¬¡: {num_rounds} | å¹¶è¡Œæ•°: 100")
    logger.info(f"\n{'QID':<5} {'CID':<5} {'Emp':<6} {'Sup':<6} {'Gui':<6} {'Saf':<6} {'Total':<8}")
    logger.info("-"*80)
    
    results = []
    
    def score_one_round(candidate, round_idx):
        prompt = GPT_SCORING_PROMPT.format(dialogue=candidate['dialogue'])
        score_text = call_api(GPT_MODEL, prompt)
        if score_text:
            return parse_scores(score_text)
        return None
    
    # å¯¹æ¯ä¸ªå€™é€‰è¿›è¡Œå¤šè½®è¯„åˆ†
    for candidate in candidates:
        all_scores = []
        
        # å¹¶è¡Œè¯„åˆ†å¤šè½®
        with ThreadPoolExecutor(max_workers=100) as executor:
            futures = [executor.submit(score_one_round, candidate, i) for i in range(num_rounds)]
            for future in as_completed(futures):
                score = future.result()
                if score:
                    all_scores.append(score)
        
        # è®¡ç®—å¹³å‡åˆ†
        if all_scores:
            avg_scores = {}
            for key in ['Empathy', 'Supportiveness', 'Guidance', 'Safety']:
                scores_list = [s.get(key, 0.0) for s in all_scores]
                avg_scores[key] = sum(scores_list) / len(scores_list) if scores_list else 0.0
            
            avg_scores['Total'] = sum(avg_scores.values())
            
            logger.info(f"{candidate['question_id']:<5} {candidate['candidate_id']:<5} "
                       f"{avg_scores['Empathy']:<6.2f} {avg_scores['Supportiveness']:<6.2f} "
                       f"{avg_scores['Guidance']:<6.2f} {avg_scores['Safety']:<6.2f} "
                       f"{avg_scores['Total']:<8.2f}")
            
            results.append({
                "question_id": candidate['question_id'],
                "question": candidate['question'],
                "candidate_id": candidate['candidate_id'],
                "dialogue": candidate['dialogue'],
                "scores": avg_scores,
                "score_details": all_scores
            })
    
    save_jsonl(results, output_file)
    logger.info("-"*80)
    logger.info(f"âœ… Step 2 å®Œæˆ: {len(results)} ä¸ªå€™é€‰è¯„åˆ†å®Œæˆ\n")
    return results


# ============================================
# Step 3: é€‰æ‹©Top-K
# ============================================

def step3_selection(scored_candidates: List[Dict], top_k: int, output_file: str):
    """Step 3: æ ¹æ®æ€»åˆ†é€‰æ‹©Top-K"""
    logger.info("\n" + "="*80)
    logger.info("Step 3: Top-K Selection")
    logger.info("="*80)
    
    # æŒ‰æ€»åˆ†æ’åº
    sorted_candidates = sorted(
        scored_candidates,
        key=lambda x: x['scores']['Total'],
        reverse=True
    )
    
    # é€‰æ‹©Top-K
    top_results = sorted_candidates[:top_k]
    
    # è¡¨æ ¼åŒ–è¾“å‡º
    logger.info(f"\n{'Rank':<6} {'QID':<5} {'CID':<5} {'Emp':<6} {'Sup':<6} {'Gui':<6} {'Saf':<6} {'Total':<8} {'Question':<30}")
    logger.info("-"*80)
    for rank, item in enumerate(top_results, 1):
        q_short = item['question'][:27] + '...' if len(item['question']) > 30 else item['question']
        logger.info(f"#{rank:<5} {item['question_id']:<5} {item['candidate_id']:<5} "
                   f"{item['scores']['Empathy']:<6.2f} {item['scores']['Supportiveness']:<6.2f} "
                   f"{item['scores']['Guidance']:<6.2f} {item['scores']['Safety']:<6.2f} "
                   f"{item['scores']['Total']:<8.2f} {q_short:<30}")
    
    save_jsonl(top_results, output_file)
    logger.info("-"*80)
    logger.info(f"âœ… Step 3 å®Œæˆ: Top-{len(top_results)} å·²ä¿å­˜\n")
    return top_results


# ============================================
# ä¸»å‡½æ•°
# ============================================

def main():
    global logger
    
    parser = argparse.ArgumentParser(description='è¿è¡Œå®éªŒ')
    parser.add_argument('--limit', type=int, default=10, help='é—®é¢˜æ•°é‡é™åˆ¶')
    parser.add_argument('--candidates', type=int, default=2, help='æ¯æ¡é—®é¢˜ç”Ÿæˆå€™é€‰æ•°')
    parser.add_argument('--score-rounds', type=int, default=3, help='æ¯ä¸ªå€™é€‰è¯„åˆ†æ¬¡æ•°')
    parser.add_argument('--version', type=str, default='v1', help='ç‰ˆæœ¬å·')
    parser.add_argument('--top-k', type=int, default=5, help='é€‰æ‹©Top-K')
    parser.add_argument('--input', type=str, default='inputs/questions.txt', help='è¾“å…¥æ–‡ä»¶')
    parser.add_argument('--log', type=str, default=None, help='æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰')
    
    args = parser.parse_args()
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    # å¦‚æœåœ¨SSHæœåŠ¡å™¨ä¸Šï¼Œä½¿ç”¨ /data/zl.zhang/Block2/{version}
    # å¦‚æœåœ¨æœ¬åœ°ï¼Œä½¿ç”¨ ./outputs/{version}
    if os.path.exists('/data'):
        output_dir = os.path.join('Outputs', args.version)
    else:
        output_dir = os.path.join('Outputs', args.version)
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
    if args.log is None:
        args.log = os.path.join(output_dir, f'experiment_{args.version}.log')
    
    # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    logger = setup_logger(args.log)
    
    # æ—¥å¿—å¤´
    logger.info("="*80)
    logger.info(f"ğŸ§ª å®éªŒé…ç½® [{args.version}]")
    logger.info("="*80)
    logger.info(f"è¾“å…¥: {args.input} | é—®é¢˜æ•°: {args.limit}")
    logger.info(f"å€™é€‰æ•°: {args.candidates} | è¯„åˆ†è½®æ¬¡: {args.score_rounds} | Top-K: {args.top_k}")
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    logger.info("="*80)
    
    # åŠ è½½æ•°æ®
    questions = load_questions(args.input, args.limit)
    logger.info(f"\nâœ… å·²åŠ è½½ {len(questions)} ä¸ªé—®é¢˜")
    
    # Step 1: Qwenç”Ÿæˆ
    candidates = step1_qwen_generation(
        questions,
        args.candidates,
        os.path.join(output_dir, f"qwen_candidates_{args.version}.jsonl")
    )
    
    # Step 2: GPTè¯„åˆ†
    scored_candidates = step2_gpt_scoring(
        candidates,
        args.score_rounds,
        os.path.join(output_dir, f"gpt_scores_{args.version}.jsonl")
    )
    
    # Step 3: é€‰æ‹©
    top_results = step3_selection(
        scored_candidates,
        args.top_k,
        os.path.join(output_dir, f"top_results_{args.version}.jsonl")
    )
    
    logger.info("\n" + "="*80)
    logger.info("ğŸ‰ å®éªŒå®Œæˆï¼")
    logger.info("="*80)
    logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
    logger.info(f"  - qwen_candidates_{args.version}.jsonl")
    logger.info(f"  - gpt_scores_{args.version}.jsonl")
    logger.info(f"  - top_results_{args.version}.jsonl")
    logger.info(f"  - experiment_{args.version}.log")
    logger.info("="*80)


if __name__ == "__main__":
    main()

