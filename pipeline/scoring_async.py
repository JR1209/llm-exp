"""
Step 2: GPT å¤šè½®è¯„åˆ† - å¼‚æ­¥ç‰ˆæœ¬
"""
import logging
import asyncio
from typing import List, Dict

from config_async import client, GPT_MODEL, build_evaluation_prompt
from core.schemas import EvaluationOutput

logger = logging.getLogger('experiment')


async def call_api_structured_async(model: str, prompt: str, schema_class, max_retries: int = 3):
    """å¼‚æ­¥è°ƒç”¨API"""
    for attempt in range(max_retries):
        try:
            response = await client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.7,
                max_tokens=2000
            )
            
            json_str = response.choices[0].message.content
            result = schema_class.model_validate_json(json_str)
            logger.debug(f"APIè°ƒç”¨æˆåŠŸ [{model}]: JSON è¾“å‡ºå·²éªŒè¯")
            return result
                
        except Exception as e:
            logger.warning(f"APIè°ƒç”¨å¼‚å¸¸ [{model}] (å°è¯• {attempt+1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(2)
    
    logger.error(f"APIè°ƒç”¨æœ€ç»ˆå¤±è´¥ [{model}]")
    return None


async def score_one_round_async(candidate, round_idx):
    """å¼‚æ­¥å•è½®è¯„åˆ†"""
    output = candidate.get('output', {})
    dialogue = output.get('dialogue', [])
    
    if isinstance(dialogue, list):
        dialogue_str = '\n'.join([f"{msg['role']}: {msg['content']}" for msg in dialogue])
    else:
        dialogue_str = str(dialogue)
    
    prompt = build_evaluation_prompt(dialogue_str)
    result = await call_api_structured_async(GPT_MODEL, prompt, EvaluationOutput)
    
    if result:
        return result.dict()
    return None


async def step2_gpt_scoring_async(
    candidates: List[Dict], 
    num_rounds: int,
    scoring_mode: str = 'per_turn',
    scoring_prompt: str = None,
    top_k: int = None
) -> List[Dict]:
    """
    Step 2: ä½¿ç”¨GPTå¼‚æ­¥è¯„åˆ†
    
    Args:
        candidates: å€™é€‰åˆ—è¡¨
        num_rounds: è¯„åˆ†è½®æ¬¡
        scoring_mode: 'per_turn' æˆ– 'overall' (ä¿ç•™å‚æ•°ï¼Œå®é™…ç”±å¤–éƒ¨è·¯ç”±)
        scoring_prompt: è‡ªå®šä¹‰è¯„åˆ†prompt
        top_k: æ¯ä¸ªé—®é¢˜ä¿ç•™å‰Kä¸ªï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ä¿ç•™ï¼‰
    """
    logger.info("\n" + "="*80)
    logger.info("Step 2: GPT Multi-round Scoring (Async)")
    logger.info("="*80)
    logger.info(f"å€™é€‰æ•°: {len(candidates)} | è¯„åˆ†è½®æ¬¡: {num_rounds}")
    logger.info(f"\n{'QID':<5} {'CID':<5} {'Emp':<6} {'Sup':<6} {'Gui':<6} {'Saf':<6} {'Total':<8}")
    logger.info("-"*80)
    
    results = []
    
    # å¯¹æ¯ä¸ªå€™é€‰è¿›è¡Œå¤šè½®è¯„åˆ†
    for candidate in candidates:
        # å¼‚æ­¥å¹¶å‘è¯„åˆ†å¤šè½®
        tasks = [score_one_round_async(candidate, i) for i in range(num_rounds)]
        all_scores = await asyncio.gather(*tasks)
        
        # è¿‡æ»¤None
        all_scores = [s for s in all_scores if s is not None]
        
        # è®¡ç®—å¹³å‡åˆ†
        if all_scores:
            avg_scores = {}
            for key in ['Empathy', 'Supportiveness', 'Guidance', 'Safety']:
                scores_list = [s.get(key, 0.0) for s in all_scores]
                avg_scores[key] = sum(scores_list) / len(scores_list) if scores_list else 0.0
            
            avg_scores['Total'] = sum(avg_scores.values())
            
            logger.info(f"{candidate['question_id']:<5} {candidate['candidate_id']:<5} "
                       f"{avg_scores['Empathy']:<6.2f} {avg_scores['Supportiveness']:<6.2f} "
                       f"{avg_scores['Guidance']:<6.2f} {avg_scores['Safety']:<6.2f} "
                       f"{avg_scores['Total']:<8.2f}")
            
            results.append({
                "question_id": candidate['question_id'],
                "question": candidate['question'],
                "candidate_id": candidate['candidate_id'],
                "output": candidate.get('output', {}),
                "scores": avg_scores,
                "score_details": all_scores
            })
    
    logger.info("-"*80)
    logger.info(f"âœ… Step 2 å®Œæˆ: {len(results)} ä¸ªå€™é€‰è¯„åˆ†å®Œæˆ\n")
    
    # Top-Kç­›é€‰ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if top_k is not None and top_k > 0:
        # æŒ‰é—®é¢˜åˆ†ç»„
        by_question = {}
        for item in results:
            qid = item['question_id']
            if qid not in by_question:
                by_question[qid] = []
            by_question[qid].append(item)
        
        # æ¯ä¸ªé—®é¢˜ä¿ç•™Top-K
        filtered_results = []
        for qid, items in by_question.items():
            sorted_items = sorted(items, key=lambda x: x['scores']['Total'], reverse=True)
            filtered_results.extend(sorted_items[:top_k])
        
        logger.info(f"ğŸ“Š Top-Kç­›é€‰: {len(results)} â†’ {len(filtered_results)}")
        return filtered_results
    
    return results
