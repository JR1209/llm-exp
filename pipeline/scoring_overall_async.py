"""
Step 2B: æ•´ä½“æ‰“åˆ† - å¼‚æ­¥ç‰ˆæœ¬
å¯¹å®Œæ•´å¯¹è¯JSONè¿›è¡Œæ•´ä½“è¯„ä¼°
"""
import logging
import asyncio
import json
from typing import List, Dict

from config_async import client, GPT_MODEL, build_overall_evaluation_prompt
from core.schemas import EvaluationOutput

logger = logging.getLogger('experiment')


async def call_scoring_api_async(model: str, prompt: str, max_retries: int = 3):
    """å¼‚æ­¥è°ƒç”¨è¯„åˆ†API"""
    for attempt in range(max_retries):
        try:
            response = await client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.3,
                max_tokens=500
            )
            
            json_str = response.choices[0].message.content
            result = EvaluationOutput.model_validate_json(json_str)
            return result
                
        except Exception as e:
            logger.warning(f"è¯„åˆ†APIè°ƒç”¨å¼‚å¸¸ (å°è¯• {attempt+1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(2)
    
    logger.error(f"è¯„åˆ†APIè°ƒç”¨æœ€ç»ˆå¤±è´¥")
    return None


async def score_one_overall_async(candidate: Dict, scoring_prompt: str = None, num_rounds: int = 3):
    """
    å¯¹å•ä¸ªå€™é€‰å¯¹è¯è¿›è¡Œæ•´ä½“è¯„åˆ†ï¼ˆå¤šè½®æ±‚å¹³å‡ï¼‰
    
    Args:
        candidate: å€™é€‰å¯¹è¯æ•°æ®
        scoring_prompt: è‡ªå®šä¹‰è¯„åˆ†promptï¼ˆå¯é€‰ï¼‰
        num_rounds: è¯„åˆ†è½®æ¬¡
    """
    dialogue_json = json.dumps(candidate['output'], ensure_ascii=False, indent=2)
    
    # ä½¿ç”¨è‡ªå®šä¹‰promptæˆ–é»˜è®¤prompt
    if scoring_prompt:
        prompt = scoring_prompt.format(dialogue_json=dialogue_json)
    else:
        prompt = build_overall_evaluation_prompt(dialogue_json)
    
    # å¤šè½®è¯„åˆ†
    scores_list = []
    for round_idx in range(num_rounds):
        result = await call_scoring_api_async(GPT_MODEL, prompt)
        if result:
            scores_list.append({
                'Empathy': result.Empathy,
                'Supportiveness': result.Supportiveness,
                'Guidance': result.Guidance,
                'Safety': result.Safety
            })
    
    if not scores_list:
        return None
    
    # è®¡ç®—å¹³å‡åˆ†
    avg_scores = {
        'Empathy': sum(s['Empathy'] for s in scores_list) / len(scores_list),
        'Supportiveness': sum(s['Supportiveness'] for s in scores_list) / len(scores_list),
        'Guidance': sum(s['Guidance'] for s in scores_list) / len(scores_list),
        'Safety': sum(s['Safety'] for s in scores_list) / len(scores_list)
    }
    avg_scores['Total'] = sum(avg_scores.values())
    
    return {
        **candidate,
        'scores': avg_scores,
        'score_details': scores_list  # ä¿ç•™æ¯è½®çš„è¯¦ç»†åˆ†æ•°
    }


async def step2_overall_scoring_async(
    candidates: List[Dict], 
    scoring_prompt: str = None,
    score_rounds: int = 3,
    top_k: int = None
) -> List[Dict]:
    """
    Step 2: æ•´ä½“æ‰“åˆ†ï¼ˆå¼‚æ­¥ï¼‰
    
    Args:
        candidates: å€™é€‰å¯¹è¯åˆ—è¡¨
        scoring_prompt: è‡ªå®šä¹‰è¯„åˆ†prompt
        score_rounds: æ¯ä¸ªå€™é€‰è¯„åˆ†è½®æ¬¡
        top_k: æ¯ä¸ªé—®é¢˜ä¿ç•™å‰Kä¸ªç»“æœï¼ˆNoneè¡¨ç¤ºä¿ç•™å…¨éƒ¨ï¼‰
    """
    logger.info("\n" + "="*80)
    logger.info("Step 2: Overall Scoring (Async)")
    logger.info("="*80)
    logger.info(f"å€™é€‰æ•°: {len(candidates)} | è¯„åˆ†è½®æ¬¡: {score_rounds} | Top-K: {top_k or 'å…¨éƒ¨'}")
    logger.info(f"\n{'QID':<5} {'CID':<5} {'Emp':<6} {'Sup':<6} {'Gui':<6} {'Saf':<6} {'Total':<7}")
    logger.info("-"*80)
    
    # å¼‚æ­¥å¹¶å‘è¯„åˆ†
    tasks = [score_one_overall_async(c, scoring_prompt, score_rounds) for c in candidates]
    scored_results = await asyncio.gather(*tasks)
    
    # è¿‡æ»¤å¤±è´¥çš„ç»“æœ
    scored_candidates = [r for r in scored_results if r is not None]
    
    # æ˜¾ç¤ºè¯„åˆ†ç»“æœ
    for item in scored_candidates:
        scores = item['scores']
        logger.info(
            f"{item['question_id']:<5} {item['candidate_id']:<5} "
            f"{scores['Empathy']:<6.2f} {scores['Supportiveness']:<6.2f} "
            f"{scores['Guidance']:<6.2f} {scores['Safety']:<6.2f} "
            f"{scores['Total']:<7.2f}"
        )
    
    logger.info("-"*80)
    logger.info(f"âœ… Step 2 å®Œæˆ: {len(scored_candidates)} ä¸ªå€™é€‰è¯„åˆ†å®Œæˆ\n")
    
    # Top-Kç­›é€‰ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if top_k is not None and top_k > 0:
        # æŒ‰é—®é¢˜åˆ†ç»„
        by_question = {}
        for item in scored_candidates:
            qid = item['question_id']
            if qid not in by_question:
                by_question[qid] = []
            by_question[qid].append(item)
        
        # æ¯ä¸ªé—®é¢˜ä¿ç•™Top-K
        filtered_results = []
        for qid, items in by_question.items():
            sorted_items = sorted(items, key=lambda x: x['scores']['Total'], reverse=True)
            filtered_results.extend(sorted_items[:top_k])
        
        logger.info(f"ğŸ“Š Top-Kç­›é€‰: {len(scored_candidates)} â†’ {len(filtered_results)}")
        return filtered_results
    
    return scored_candidates
