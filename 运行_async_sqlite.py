#!/data/zl.zhang/Code/venv/bin/python3
"""
å®éªŒä¸»è„šæœ¬ - å¼‚æ­¥ç‰ˆæœ¬ + MLflow + SQLite
"""
import argparse
import asyncio
import os
import sys
from pathlib import Path
import logging
import mlflow
import json
from datetime import datetime

PROJECT_ROOT = Path(__file__).parent.absolute()
sys.path.insert(0, str(PROJECT_ROOT))

from utils.io_handler import (
    load_questions,
    save_json,
    format_generation_output,
    format_scoring_output,
    format_final_output
)
from pipeline.generation_async import step1_qwen_generation_async
from pipeline.generation_dual_async import step1_dual_generation_async
from pipeline.scoring_async import step2_gpt_scoring_async
from pipeline.scoring_overall_async import step2_overall_scoring_async
from pipeline.selection import step3_selection
from sqlite_handler import SQLiteHandler, load_prompts_from_file, load_code_snapshots

def setup_logger(log_file: str = None):
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    if log_file:
        Path(log_file).parent.mkdir(parents=True, exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8') if log_file else logging.NullHandler(),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger('experiment')

async def main_async(args):
    """å¼‚æ­¥ä¸»å‡½æ•° - é›†æˆ SQLite + MLflow"""
    # åˆå§‹åŒ– SQLite
    db = SQLiteHandler(args.db_path)
    logger = None
    
    try:
        # é…ç½® MLflow
        mlflow.set_tracking_uri("sqlite:///mlflow.db")
        mlflow.set_experiment("ESC_Experiments")
        
        # å¯åŠ¨ MLflow run
        with mlflow.start_run(run_name=args.version):
            # è®¾ç½®è¾“å‡ºç›®å½•
            output_dir = PROJECT_ROOT / 'Outputs' / args.version
            output_dir.mkdir(parents=True, exist_ok=True)
            output_dir = str(output_dir)
            
            # è®¾ç½®æ—¥å¿—ï¼ˆä¿å­˜åˆ° logs ç›®å½•ï¼‰
            logs_dir = PROJECT_ROOT / 'logs'
            logs_dir.mkdir(exist_ok=True)
            if args.log is None:
                args.log = str(logs_dir / f'experiment_{args.version}.log')
            logger = setup_logger(args.log)
            
            # æ—¥å¿—å¤´
            logger.info("="*80)
            logger.info(f"ğŸ§ª å®éªŒé…ç½® [{args.version}] - å¼‚æ­¥ç‰ˆæœ¬ + MLflow + SQLite")
            logger.info("="*80)
            logger.info(f"è¾“å…¥: {args.input} | é—®é¢˜æ•°: {args.limit}")
            logger.info(f"å€™é€‰æ•°: {args.candidates} | è¯„åˆ†è½®æ¬¡: {args.score_rounds} | Top-K: {args.top_k}")
            logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
            logger.info(f"SQLite æ•°æ®åº“: {args.db_path}")
            logger.info("="*80)
            
            # åŠ è½½é—®é¢˜
            questions = load_questions(args.input, args.limit)
            logger.info(f"\nâœ… å·²åŠ è½½ {len(questions)} ä¸ªé—®é¢˜")
            
            # åŠ è½½ prompts å’Œä»£ç å¿«ç…§
            logger.info("ğŸ“ åŠ è½½ prompts å’Œä»£ç å¿«ç…§...")
            prompts = load_prompts_from_file('prompts.json')
            code_snapshots = load_code_snapshots()
            
            # å®éªŒé…ç½®
            config = {
                "limit": args.limit,
                "candidates": args.candidates,
                "score_rounds": args.score_rounds,
                "top_k": args.top_k,
                "input_file": args.input
            }
            
            # è·å– Git ä¿¡æ¯
            git_info = None
            try:
                import subprocess
                git_commit = subprocess.check_output(
                    ['git', 'rev-parse', 'HEAD'],
                    cwd=PROJECT_ROOT
                ).decode('utf-8').strip()
                
                git_branch = subprocess.check_output(
                    ['git', 'rev-parse', '--abbrev-ref', 'HEAD'],
                    cwd=PROJECT_ROOT
                ).decode('utf-8').strip()
                
                git_status = subprocess.check_output(
                    ['git', 'status', '--porcelain'],
                    cwd=PROJECT_ROOT
                ).decode('utf-8').strip()
                
                git_info = {
                    'commit': git_commit,
                    'branch': git_branch,
                    'is_dirty': str(len(git_status) > 0)
                }
            except:
                pass
            
            # ä¿å­˜å®éªŒåˆ° SQLiteï¼ˆåˆå§‹çŠ¶æ€ï¼‰
            logger.info("ğŸ’¾ ä¿å­˜å®éªŒå…ƒæ•°æ®åˆ° SQLite...")
            db.save_experiment(
                version=args.version,
                config=config,
                input_questions=questions,
                prompts=prompts,
                code_snapshots=code_snapshots,
                git_info=git_info
            )
            if git_info:
                logger.info(f"âœ… å®éªŒå…ƒæ•°æ®å·²ä¿å­˜åˆ° SQLite (Git: {git_info['commit'][:8]})")
            else:
                logger.info("âœ… å®éªŒå…ƒæ•°æ®å·²ä¿å­˜åˆ° SQLite (æ—  Git ä¿¡æ¯)")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ğŸ“¦ è®°å½•å®Œæ•´å¿«ç…§åˆ° MLflow
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            logger.info("\nğŸ“¦ å¼€å§‹è®°å½•å®Œæ•´å¿«ç…§åˆ° MLflow...")
            
            # 1ï¸âƒ£ è®°å½•å®éªŒå‚æ•°
            logger.info("  â”œâ”€ è®°å½•å®éªŒå‚æ•°...")
            mlflow.log_params(config)
            mlflow.log_param("database", args.db_path)
            mlflow.log_metric("num_questions", len(questions))
            
            # 2ï¸âƒ£ è®°å½• Git ç‰ˆæœ¬ä¿¡æ¯
            logger.info("  â”œâ”€ è®°å½• Git ç‰ˆæœ¬...")
            try:
                import subprocess
                
                git_commit = subprocess.check_output(
                    ['git', 'rev-parse', 'HEAD'],
                    cwd=PROJECT_ROOT
                ).decode('utf-8').strip()
                
                git_branch = subprocess.check_output(
                    ['git', 'rev-parse', '--abbrev-ref', 'HEAD'],
                    cwd=PROJECT_ROOT
                ).decode('utf-8').strip()
                
                git_status = subprocess.check_output(
                    ['git', 'status', '--porcelain'],
                    cwd=PROJECT_ROOT
                ).decode('utf-8').strip()
                
                is_dirty = len(git_status) > 0
                
                mlflow.log_param("git_commit", git_commit[:8])
                mlflow.set_tag("git.commit", git_commit)
                mlflow.set_tag("git.branch", git_branch)
                mlflow.set_tag("git.is_dirty", str(is_dirty))
                
                if is_dirty:
                    logger.warning("     âš ï¸  è­¦å‘Š: ä»£ç æœ‰æœªæäº¤çš„æ›´æ”¹ï¼")
                    mlflow.set_tag("git.warning", "Uncommitted changes detected")
                    
                    git_diff = subprocess.check_output(
                        ['git', 'diff'],
                        cwd=PROJECT_ROOT
                    ).decode('utf-8')
                    
                    diff_file = Path(output_dir) / "git_diff.patch"
                    with open(diff_file, 'w', encoding='utf-8') as f:
                        f.write(git_diff)
                    mlflow.log_artifact(str(diff_file), artifact_path="code")
                
                logger.info(f"     âœ“ Git: {git_commit[:8]} ({git_branch})")
                
            except Exception as e:
                logger.warning(f"     âš ï¸  Git ä¿¡æ¯è·å–å¤±è´¥: {e}")
                mlflow.set_tag("git.error", str(e))
            
            # 3ï¸âƒ£ è®°å½•ä»£ç å¿«ç…§ï¼ˆä»…æ ¸å¿ƒæ–‡ä»¶ï¼‰
            logger.info("  â”œâ”€ è®°å½•ä»£ç å¿«ç…§...")
            import shutil
            
            # å®šä¹‰éœ€è¦è®°å½•çš„æ ¸å¿ƒæ–‡ä»¶
            core_files = [
                'pipeline/generation_async.py',
                'pipeline/scoring_async.py', 
                'pipeline/selection.py',
                'è¿è¡Œ_async_sqlite.py'
            ]
            
            # å¯é€‰ï¼šconfig æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if (PROJECT_ROOT / 'config.py').exists():
                core_files.append('config.py')
            
            # ç›´æ¥è®°å½•æ–‡ä»¶ï¼Œä¸åˆ›å»º code_snapshot ç›®å½•
            for file_path in core_files:
                full_path = PROJECT_ROOT / file_path
                if full_path.exists():
                    mlflow.log_artifact(str(full_path), artifact_path="code")
            
            logger.info(f"     âœ“ å·²ä¿å­˜ {len(core_files)} ä¸ªæ ¸å¿ƒä»£ç æ–‡ä»¶")
            
            # 4ï¸âƒ£ è®°å½• Promptsï¼ˆç›´æ¥è®°å½•æºæ–‡ä»¶ï¼‰
            logger.info("  â”œâ”€ è®°å½• Prompts...")
            prompts_source_file = PROJECT_ROOT / 'prompts.json'
            if prompts_source_file.exists():
                mlflow.log_artifact(str(prompts_source_file), artifact_path="config")
                logger.info(f"     âœ“ å·²ä¿å­˜ prompts.json")
            
            # å¦‚æœæœ‰å…¶ä»–é…ç½®æ–‡ä»¶ä¹Ÿå¯ä»¥åŠ ä¸Š
            if (PROJECT_ROOT / 'config.py').exists():
                mlflow.log_artifact(str(PROJECT_ROOT / 'config.py'), artifact_path="config")
            
            # 5ï¸âƒ£ è®°å½•è¾“å…¥æ•°æ®ï¼ˆinputs ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼‰
            logger.info("  â”œâ”€ è®°å½•è¾“å…¥æ•°æ®...")
            
            # è®°å½•ä¸»è¾“å…¥æ–‡ä»¶
            if Path(args.input).exists():
                mlflow.log_artifact(args.input, artifact_path="inputs")
            
            # è®°å½• inputs ç›®å½•ä¸‹çš„å…¶ä»–æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            inputs_dir = PROJECT_ROOT / 'inputs'
            if inputs_dir.exists():
                input_files = list(inputs_dir.glob('*'))
                input_files = [f for f in input_files if f.is_file()]  # åªè¦æ–‡ä»¶ï¼Œä¸è¦ç›®å½•
                
                for input_file in input_files:
                    # é¿å…é‡å¤è®°å½•ä¸»è¾“å…¥æ–‡ä»¶
                    if str(input_file) != str(Path(args.input).absolute()):
                        mlflow.log_artifact(str(input_file), artifact_path="inputs")
                
                logger.info(f"     âœ“ å·²ä¿å­˜ {len(input_files)} ä¸ªè¾“å…¥æ–‡ä»¶")
            else:
                logger.info(f"     âœ“ å·²ä¿å­˜è¾“å…¥æ–‡ä»¶: {Path(args.input).name}")
            
            logger.info("  â””â”€ å¿«ç…§è®°å½•å®Œæˆï¼\n")
            
            # Step 1: ç”Ÿæˆå€™é€‰ç­”æ¡ˆ (æ ¹æ®æ¨¡å¼é€‰æ‹©)
            logger.info("\n" + "="*80)
            logger.info("ğŸ”„ Step 1: ç”Ÿæˆå€™é€‰ç­”æ¡ˆ")
            logger.info("="*80)
            
            # åŠ è½½è‡ªå®šä¹‰promptï¼ˆå¦‚æœæä¾›ï¼‰
            generation_prompt = None
            if args.generation_prompt_file and Path(args.generation_prompt_file).exists():
                with open(args.generation_prompt_file, 'r', encoding='utf-8') as f:
                    generation_prompt = f.read()
                logger.info(f"ğŸ“ ä½¿ç”¨è‡ªå®šä¹‰ç”ŸæˆPrompt: {args.generation_prompt_file}")
            
            if args.mode == 'dual':
                # åŒæ¨¡å‹å¯¹è¯æ¨¡å¼
                logger.info(f"æ¨¡å¼: åŒæ¨¡å‹å¯¹è¯ | User: {args.user_model} | Agent: {args.agent_model} | è½®æ•°: {args.dialogue_rounds}")
                candidates = await step1_dual_generation_async(
                    questions, 
                    args.user_model,
                    args.agent_model,
                    args.candidates,
                    args.dialogue_rounds
                )
            else:
                # å•æ¨¡å‹ç”Ÿæˆæ¨¡å¼
                logger.info(f"æ¨¡å¼: å•æ¨¡å‹ç”Ÿæˆ | å¯¹è¯è½®æ•°: {args.num_turns}")
                candidates = await step1_qwen_generation_async(questions, args.candidates, args.num_turns)
            
            # ä¿å­˜Step1ç»“æœåˆ°æ–‡ä»¶
            raw_file = os.path.join(output_dir, f"qwen_candidates_raw_{args.version}.json")
            save_json(candidates, raw_file)
            logger.info(f"ğŸ’¾ å·²ä¿å­˜åŸå§‹æ•°æ®: {raw_file}")
            
            formatted_gen = format_generation_output(candidates)
            gen_file = os.path.join(output_dir, f"1_generation_{args.version}.json")
            save_json(formatted_gen, gen_file)
            logger.info(f"ğŸ’¾ å·²ä¿å­˜ç”Ÿæˆç»“æœ: {gen_file}")
            
            # æ›´æ–° SQLite - Step1è¾“å‡º
            logger.info("ğŸ’¾ ä¿å­˜ Step1 ç»“æœåˆ° SQLite...")
            db.update_experiment_outputs(
                version=args.version,
                step1_generation=formatted_gen
            )
            
            mlflow.log_metric("num_candidates_generated", len(candidates))
            
            # Step 2: è¯„åˆ† (æ ¹æ®æ¨¡å¼é€‰æ‹©)
            logger.info("\n" + "="*80)
            logger.info("ğŸ”„ Step 2: è¯„åˆ†")
            logger.info("="*80)
            
            # åŠ è½½è‡ªå®šä¹‰promptï¼ˆå¦‚æœæä¾›ï¼‰
            scoring_prompt = None
            if args.scoring_prompt_file and Path(args.scoring_prompt_file).exists():
                with open(args.scoring_prompt_file, 'r', encoding='utf-8') as f:
                    scoring_prompt = f.read()
                logger.info(f"ğŸ“ ä½¿ç”¨è‡ªå®šä¹‰æ‰“åˆ†Prompt: {args.scoring_prompt_file}")
            
            if args.scoring_mode == 'overall':
                # æ•´ä½“æ‰“åˆ†æ¨¡å¼
                logger.info(f"æ¨¡å¼: æ•´ä½“æ‰“åˆ† | æ¨¡å‹: {args.scoring_model} | Top-K: {args.scoring_top_k or 'å…¨éƒ¨'}")
                scored_candidates = await step2_overall_scoring_async(
                    candidates,
                    scoring_prompt=scoring_prompt,
                    score_rounds=args.score_rounds,
                    top_k=args.scoring_top_k
                )
            else:
                # é€è½®æ‰“åˆ†æ¨¡å¼
                logger.info(f"æ¨¡å¼: é€è½®æ‰“åˆ† | æ¨¡å‹: {args.scoring_model} | Top-K: {args.scoring_top_k or 'å…¨éƒ¨'}")
                scored_candidates = await step2_gpt_scoring_async(
                    candidates,
                    args.score_rounds,
                    scoring_mode=args.scoring_mode,
                    scoring_prompt=scoring_prompt,
                    top_k=args.scoring_top_k
                )
            
            # ä¿å­˜Step2ç»“æœåˆ°æ–‡ä»¶
            raw_scores_file = os.path.join(output_dir, f"gpt_scores_raw_{args.version}.json")
            save_json(scored_candidates, raw_scores_file)
            logger.info(f"ğŸ’¾ å·²ä¿å­˜åŸå§‹è¯„åˆ†: {raw_scores_file}")
            
            formatted_scores = format_scoring_output(scored_candidates)
            scores_file = os.path.join(output_dir, f"2_scores_{args.version}.json")
            save_json(formatted_scores, scores_file)
            logger.info(f"ğŸ’¾ å·²ä¿å­˜è¯„åˆ†ç»“æœ: {scores_file}")
            
            # æ›´æ–° SQLite - Step2è¾“å‡º
            logger.info("ğŸ’¾ ä¿å­˜ Step2 ç»“æœåˆ° SQLite...")
            db.update_experiment_outputs(
                version=args.version,
                step2_scores=formatted_scores
            )
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            if scored_candidates:
                avg_empathy = sum(c['scores']['Empathy'] for c in scored_candidates) / len(scored_candidates)
                avg_supportiveness = sum(c['scores']['Supportiveness'] for c in scored_candidates) / len(scored_candidates)
                avg_guidance = sum(c['scores']['Guidance'] for c in scored_candidates) / len(scored_candidates)
                avg_safety = sum(c['scores']['Safety'] for c in scored_candidates) / len(scored_candidates)
                avg_total = sum(c['scores']['Total'] for c in scored_candidates) / len(scored_candidates)
                
                statistics = {
                    "avg_empathy": avg_empathy,
                    "avg_supportiveness": avg_supportiveness,
                    "avg_guidance": avg_guidance,
                    "avg_safety": avg_safety,
                    "avg_total_score": avg_total,
                    "num_candidates": len(scored_candidates)
                }
                
                mlflow.log_metrics({
                    "avg_empathy": avg_empathy,
                    "avg_supportiveness": avg_supportiveness,
                    "avg_guidance": avg_guidance,
                    "avg_safety": avg_safety,
                    "avg_total_score": avg_total
                })
                
                logger.info(f"\nğŸ“Š å¹³å‡åˆ†æ•°:")
                logger.info(f"  Empathy: {avg_empathy:.2f}")
                logger.info(f"  Supportiveness: {avg_supportiveness:.2f}")
                logger.info(f"  Guidance: {avg_guidance:.2f}")
                logger.info(f"  Safety: {avg_safety:.2f}")
                logger.info(f"  Total: {avg_total:.2f}")
            else:
                statistics = {}
            
            # Step 3: ç”Ÿæˆæœ€ç»ˆç»“æœ
            logger.info("\n" + "="*80)
            logger.info("ğŸ”„ Step 3: ç”Ÿæˆæœ€ç»ˆç»“æœ")
            logger.info("="*80)
            final_results = format_final_output(scored_candidates)
            final_file = os.path.join(output_dir, f"3_final_results_{args.version}.json")
            save_json(final_results, final_file)
            logger.info(f"ğŸ’¾ å·²ä¿å­˜æœ€ç»ˆç»“æœ: {final_file}")
            
            # æ›´æ–° SQLite - Step3è¾“å‡ºå’Œå®ŒæˆçŠ¶æ€
            logger.info("ğŸ’¾ ä¿å­˜ Step3 ç»“æœåˆ° SQLite...")
            db.update_experiment_outputs(
                version=args.version,
                step3_final=final_results,
                statistics=statistics,
                status='completed'
            )
            
            mlflow.log_metric("num_final_results", len(final_results))
            
            # 6ï¸âƒ£ è®°å½•è¾“å‡ºç»“æœåˆ° MLflowï¼ˆä»…æ ¸å¿ƒç»“æœæ–‡ä»¶ï¼‰
            logger.info("ğŸ“¦ è®°å½•è¾“å‡ºç»“æœåˆ° MLflow...")
            
            # åªè®°å½•æ ¸å¿ƒè¾“å‡ºæ–‡ä»¶
            output_files = [
                gen_file,           # 1_generation_xxx.json
                scores_file,        # 2_scores_xxx.json  
                final_file,         # 3_final_results_xxx.json
                args.log            # å®éªŒæ—¥å¿—
            ]
            
            for file_path in output_files:
                if Path(file_path).exists():
                    mlflow.log_artifact(file_path, artifact_path="outputs")
            
            logger.info(f"  âœ“ å·²è®°å½• {len(output_files)} ä¸ªæ ¸å¿ƒè¾“å‡ºæ–‡ä»¶")
            
            # 7ï¸âƒ£ è®°å½•å®éªŒæ‘˜è¦ï¼ˆä½¿ç”¨ MLflow çš„ dict åŠŸèƒ½ï¼‰
            logger.info("\nğŸ“Š è®°å½•å®éªŒæ‘˜è¦...")
            summary = {
                "version": args.version,
                "git_commit": git_info.get('commit', 'N/A') if git_info else 'N/A',
                "git_branch": git_info.get('branch', 'N/A') if git_info else 'N/A',
                "config": config,
                "statistics": statistics,
                "num_questions": len(questions),
                "num_prompts": len(prompts) if prompts else 0,
                "timestamp": datetime.now().isoformat()
            }
            
            # ç›´æ¥ç”¨ MLflow çš„ log_dictï¼Œä¸ä¿å­˜åˆ°æ–‡ä»¶
            mlflow.log_dict(summary, "summary/experiment_summary.json")
            logger.info("  âœ“ å®éªŒæ‘˜è¦å·²è®°å½•")
            
            # å®Œæˆ
            logger.info("\n" + "="*80)
            logger.info("ğŸ‰ å®éªŒå®Œæˆï¼")
            logger.info("="*80)
            logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
            logger.info(f"ğŸ“Š MLflow Run ID: {mlflow.active_run().info.run_id}")
            logger.info(f"ğŸ’¾ SQLite æ•°æ®åº“: {args.db_path}")
            logger.info(f"ğŸ’¾ å®éªŒç‰ˆæœ¬: {args.version}")
            logger.info("="*80)
            
    except Exception as e:
        if logger:
            logger.error(f"\nâŒ å®éªŒå¤±è´¥: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
        
        # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
        try:
            db.update_experiment_outputs(
                version=args.version,
                status='failed'
            )
        except:
            pass
        
        raise
    finally:
        # å…³é—­æ•°æ®åº“è¿æ¥
        db.close()

def main():
    parser = argparse.ArgumentParser(description='è¿è¡Œå®éªŒ - å¼‚æ­¥ç‰ˆæœ¬ + MLflow + SQLite')
    parser.add_argument('--limit', type=int, default=10, help='é—®é¢˜æ•°é‡é™åˆ¶')
    parser.add_argument('--candidates', type=int, default=2, help='æ¯æ¡é—®é¢˜ç”Ÿæˆå€™é€‰æ•°')
    parser.add_argument('--score-rounds', type=int, default=3, help='æ¯ä¸ªå€™é€‰è¯„åˆ†æ¬¡æ•°')
    parser.add_argument('--version', type=str, default='v1_sqlite', help='å®éªŒç‰ˆæœ¬å·')
    parser.add_argument('--top-k', type=int, default=5, help='é€‰æ‹©Top-K')
    parser.add_argument('--input', type=str, default=str(PROJECT_ROOT / 'inputs' / 'questions.txt'), help='è¾“å…¥æ–‡ä»¶')
    parser.add_argument('--log', type=str, default=None, help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--db-path', type=str, default='experiments.db', help='SQLite æ•°æ®åº“æ–‡ä»¶è·¯å¾„')
    
    # æ–°å¢ï¼šå¯¹è¯æ¨¡å¼å‚æ•°
    parser.add_argument('--mode', type=str, default='single', choices=['single', 'dual'], help='å¯¹è¯ç”Ÿæˆæ¨¡å¼: single=å•æ¨¡å‹, dual=åŒæ¨¡å‹')
    parser.add_argument('--num-turns', type=int, default=5, help='å•æ¨¡å‹ç”Ÿæˆå¯¹è¯è½®æ•°')
    parser.add_argument('--user-model', type=str, default='qwen-max', help='åŒæ¨¡å‹æ¨¡å¼ä¸‹çš„Useræ¨¡å‹')
    parser.add_argument('--agent-model', type=str, default='gpt-4o-mini', help='åŒæ¨¡å‹æ¨¡å¼ä¸‹çš„Agentæ¨¡å‹')
    parser.add_argument('--dialogue-rounds', type=int, default=3, help='åŒæ¨¡å‹å¯¹è¯è½®æ•°')
    
    # æ–°å¢ï¼šæ‰“åˆ†æ¨¡å¼å‚æ•°
    parser.add_argument('--scoring-mode', type=str, default='per_turn', choices=['per_turn', 'overall'], help='æ‰“åˆ†æ¨¡å¼: per_turn=é€è½®æ‰“åˆ†, overall=æ•´ä½“æ‰“åˆ†')
    parser.add_argument('--scoring-model', type=str, default='gpt-4o-mini', help='æ‰“åˆ†ä½¿ç”¨çš„æ¨¡å‹')
    parser.add_argument('--scoring-top-k', type=int, default=None, help='æ¯ä¸ªé—®é¢˜ä¿ç•™å‰Kä¸ªç»“æœï¼ˆNone=å…¨éƒ¨ä¿ç•™ï¼‰')
    parser.add_argument('--generation-prompt-file', type=str, default=None, help='è‡ªå®šä¹‰ç”Ÿæˆpromptæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--scoring-prompt-file', type=str, default=None, help='è‡ªå®šä¹‰æ‰“åˆ†promptæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main_async(args))

if __name__ == "__main__":
    main()