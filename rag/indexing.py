"""
æ–‡æ¡£ç´¢å¼•è„šæœ¬
å°† documents/ ä¸‹çš„æ–‡æ¡£å‘é‡åŒ–å¹¶å­˜å‚¨åˆ° Chroma
"""

import os
import sys
from pathlib import Path
from typing import List

PROJECT_ROOT = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(PROJECT_ROOT))

from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

from config_async import API_KEY, API_BASE_URL


def load_documents(directory: str = "documents") -> List:
    """åŠ è½½æ–‡æ¡£ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶"""
    docs_path = PROJECT_ROOT / directory
    
    if not docs_path.exists():
        raise FileNotFoundError(f"æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {docs_path}")
    
    files = list(docs_path.glob("*.txt"))
    if not files:
        raise FileNotFoundError(f"æ–‡æ¡£ç›®å½•ä¸ºç©º: {docs_path}")
    
    print(f"ğŸ“‚ æ‰¾åˆ° {len(files)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
    
    loader = DirectoryLoader(
        str(docs_path),
        glob="*.txt",
        loader_cls=TextLoader,
        loader_kwargs={"encoding": "utf-8"}
    )
    
    documents = loader.load()
    print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    return documents


def split_documents(documents: List, chunk_size: int = 500, chunk_overlap: int = 50) -> List:
    """åˆ‡åˆ†æ–‡æ¡£ä¸ºå°å—"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
    )
    
    chunks = text_splitter.split_documents(documents)
    print(f"ğŸ“ æ–‡æ¡£å·²åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªå—")
    return chunks


def create_vectorstore(chunks: List, persist_directory: str = "vectorstore"):
    """åˆ›å»ºå‘é‡æ•°æ®åº“"""
    persist_path = PROJECT_ROOT / persist_directory
    
    embeddings = OpenAIEmbeddings(
        openai_api_key=API_KEY,
        openai_api_base=API_BASE_URL,
        model="text-embedding-ada-002"
    )
    
    print(f"ğŸ”„ æ­£åœ¨å‘é‡åŒ–æ–‡æ¡£...")
    
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=str(persist_path)
    )
    
    print(f"âœ… å‘é‡åº“å·²ä¿å­˜åˆ°: {persist_path}")
    return vectorstore


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ“š å¿ƒç†å’¨è¯¢æ¡ˆä¾‹å‘é‡åŒ–ç´¢å¼•")
    print("=" * 80)
    
    try:
        print("\n[æ­¥éª¤ 1/3] åŠ è½½æ–‡æ¡£...")
        documents = load_documents()
        
        print("\n[æ­¥éª¤ 2/3] åˆ‡åˆ†æ–‡æ¡£...")
        chunks = split_documents(documents, chunk_size=500, chunk_overlap=50)
        
        print("\n[æ­¥éª¤ 3/3] åˆ›å»ºå‘é‡åº“...")
        vectorstore = create_vectorstore(chunks)
        
        print("\n" + "=" * 80)
        print("ğŸ‰ ç´¢å¼•å®Œæˆï¼")
        print("=" * 80)
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  - æ–‡æ¡£æ•°é‡: {len(documents)}")
        print(f"  - æ–‡æ¡£å—æ•°: {len(chunks)}")
        print(f"  - å‘é‡åº“è·¯å¾„: {PROJECT_ROOT / 'vectorstore'}")
        print("\nç°åœ¨å¯ä»¥è¿è¡Œå®éªŒå¹¶ä½¿ç”¨ --use-rag å‚æ•°å¯ç”¨ RAG åŠŸèƒ½")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()